# **Engenharia de Dados - ANTAQ ğŸš¢ğŸ’¾**

Este repositÃ³rio contÃ©m os scripts de ETL desenvolvidos para manipulaÃ§Ã£o e anÃ¡lise de dados do **AnuÃ¡rio EstatÃ­stico da ANTAQ**. O objetivo deste projeto Ã© extrair, transformar e carregar os dados no **SQL Server**, garantindo que os analistas de BI e cientistas de dados possam acessÃ¡-los de maneira eficiente.

---

## **ğŸ“Œ AutoavaliaÃ§Ã£o**
| TÃ³pico | NÃ­vel (0-6) |
|--------|------------|
| Ferramentas de visualizaÃ§Ã£o de dados (Power BI, Qlik Sense e outros) | 6 |
| ManipulaÃ§Ã£o e tratamento de dados com Python | 6 |
| ManipulaÃ§Ã£o e tratamento de dados com PySpark | 4 |
| Desenvolvimento de data workflows em Ambiente Azure com Databricks | 3 |
| Desenvolvimento de data workflows com Airflow | 3 |
| ManipulaÃ§Ã£o de bases de dados NoSQL | 4 |
| Web crawling e web scraping para mineraÃ§Ã£o de dados | 6 |
| ConstruÃ§Ã£o de APIs: REST, SOAP e Microservices | 4 |

---

## (QuestÃ£o 2a) Escolha da Estrutura de Dados (Data Lake, SQL Server ou NoSQL)

Para armazenar os dados do **AnuÃ¡rio EstatÃ­stico da ANTAQ**, analisamos trÃªs opÃ§Ãµes: **Data Lake, SQL Server e NoSQL (MongoDB)**.

### **ğŸ“Œ Escolha Final: Data Lake + SQL Server**

Optamos por um **modelo hÃ­brido**, armazenando os dados no **Data Lake e SQL Server**.

---

### **ğŸ“Œ Justificativa para a Escolha**

1ï¸âƒ£ **Data Lake** (Armazena os dados brutos e transformados antes do carregamento no SQL Server)
   - **Motivo**: Permite armazenar **dados estruturados e semi-estruturados** sem um esquema rÃ­gido.
   - **BenefÃ­cios**:
     âœ… Maior flexibilidade para processamento e transformaÃ§Ã£o dos dados.  
     âœ… Suporte a mÃºltiplos formatos (JSON, Parquet, CSV).  
     âœ… Facilita a reprocessamento e auditoria.  

2ï¸âƒ£ **SQL Server** (Armazena os dados processados para consulta e anÃ¡lise no Power BI)
   - **Motivo**: NecessÃ¡rio para fornecer **dados estruturados e otimizados** para anÃ¡lises.  
   - **BenefÃ­cios**:
     âœ… Suporte a **transaÃ§Ãµes e integridade referencial**.  
     âœ… Melhor desempenho para consultas complexas.  
     âœ… IntegraÃ§Ã£o nativa com **Power BI** (ferramenta dos analistas do cliente).  
   - **Uso no Projeto**:
     - Criamos as tabelas `atracacao_fato` e `carga_fato` no SQL Server.
     - Implementamos Ã­ndices para otimizar consultas.

3ï¸âƒ£ **NoSQL (MongoDB) â€“ NÃ£o utilizado**  
   - **Motivo**: O banco relacional atende melhor aos requisitos do projeto.  
   - **Quando seria Ãºtil?** Para armazenar logs de eventos ou dados nÃ£o estruturados.  

---

### **ğŸ“Œ ConclusÃ£o**
âœ… **Os dados brutos e extraÃ­dos sÃ£o armazenados no Data Lake.**  
âœ… **Os dados processados sÃ£o carregados no SQL Server para otimizar a anÃ¡lise no Power BI.**  
âœ… **O NoSQL nÃ£o foi utilizado, pois o SQL Server atende melhor Ã s necessidades do projeto.**  

---
## (QuestÃ£o 2b) â€¢Criar os scripts SQL de criaÃ§Ã£o das tabelas (atracacao_fato e carga_fato) e adicionar no repositÃ³rio.
Os scripts SQL para a criaÃ§Ã£o das tabelas atracacao_fato e carga_fato podem ser encontrados no seguinte arquivo:

### ğŸ“‚ sql/criar_tabelas.sql

Este script cria a estrutura necessÃ¡ria no SQL Server para armazenar os dados processados no pipeline de ETL. As tabelas foram projetadas para armazenar informaÃ§Ãµes sobre:

atracacao_fato ğŸš¢

ID da atracaÃ§Ã£o, porto, tipo de navegaÃ§Ã£o, datas relevantes, tempo de espera e operaÃ§Ã£o, entre outras informaÃ§Ãµes.
carga_fato ğŸ“¦

ID da carga, origem/destino, mercadoria transportada, tipo de operaÃ§Ã£o, TEU, quantidade transportada e peso lÃ­quido da carga.
ğŸ“Œ Como utilizar?
Para criar as tabelas no banco de dados, basta executar o script no SQL Server utilizando um cliente como SQL Server Management Studio (SSMS) ou Azure Data Studio.

## (QuestÃ£o 2c) â€¢Criar a query otimizada para Excel que responde Ã  QuestÃ£o.
A consulta SQL otimizada para o Excel que fornece os dados solicitados sobre nÃºmero de atracaÃ§Ãµes, tempo de espera mÃ©dio e tempo atracado mÃ©dio pode ser encontrada no seguinte arquivo:

### ğŸ“‚ sql/query_economistas.sql

ğŸ“Œ O que essa consulta faz?
Filtra os dados apenas para os anos 2021 a 2023.
Agrupa as informaÃ§Ãµes por localidade (CearÃ¡, Nordeste e Brasil) e mÃªs/ano.
Calcula os seguintes indicadores:
NÃºmero de atracaÃ§Ãµes no perÃ­odo.
VariaÃ§Ã£o percentual do nÃºmero de atracaÃ§Ãµes em relaÃ§Ã£o ao mesmo mÃªs do ano anterior.
Tempo mÃ©dio de espera para atracaÃ§Ã£o.
Tempo mÃ©dio que os navios ficaram atracados.
ğŸ“Œ Como utilizar no Excel?
Executar a query no SQL Server utilizando SQL Server Management Studio (SSMS) ou Azure Data Studio.
Exportar os resultados para CSV ou Excel diretamente no SQL Server.
Abrir o arquivo no Excel para anÃ¡lise e visualizaÃ§Ã£o dos dados.
ğŸ’¡ Essa consulta foi otimizada para minimizar o nÃºmero de linhas, garantindo melhor desempenho ao abrir no Excel.

## (QuestÃ£o 3) ğŸ¤– .Criar a DAG do Airflow 

Para garantir a orquestraÃ§Ã£o do ETL dos dados da ANTAQ, implementamos um ambiente utilizando **Apache Airflow** e **Docker**.

### ğŸ“‚ **Arquivos principais**
- `docker/docker-compose.yml` â†’ ConfiguraÃ§Ã£o do ambiente Docker para Airflow.
- `airflow/dags/antaq_etl_dag.py` â†’ DAG do Airflow que executa o pipeline ETL.
- `airflow/scripts/download_dados.py` â†’ Script que baixa os dados da ANTAQ.
- `airflow/scripts/etl_pipeline.py` â†’ Script que processa os dados e armazena no SQL Server.

### âš™ï¸ **ExecuÃ§Ã£o do Workflow**
O fluxo de trabalho do Airflow executa as seguintes etapas **a cada 15 dias**:
1ï¸âƒ£ **Verificar se os dados estÃ£o disponÃ­veis** no Data Lake antes do processamento.  
2ï¸âƒ£ **Baixar os dados** da ANTAQ para o Data Lake.  
3ï¸âƒ£ **Realizar a transformaÃ§Ã£o dos dados** e inseri-los no SQL Server.  
4ï¸âƒ£ **Enviar um e-mail de confirmaÃ§Ã£o** ao final do processo.

### ğŸ›  **Como rodar o Airflow com Docker**
1. **Iniciar o ambiente**
   ```bash
   cd docker
   docker-compose up -d


## **ğŸ“ Estrutura do RepositÃ³rio**
```bash
engenharia-de-dados-antaq/
â”‚â”€â”€ airflow/                 # DAGs e scripts de ETL no Apache Airflow
â”‚   â”œâ”€â”€ data/                # DiretÃ³rio para armazenar dados baixados
â”‚   â”œâ”€â”€ dags/                # Arquivos das DAGs do Airflow
â”‚   â”œâ”€â”€ scripts/             # Scripts de ETL
â”‚   â”œâ”€â”€ config/              # Arquivos de configuraÃ§Ã£o
â”‚â”€â”€ docker/                  # ConfiguraÃ§Ã£o para rodar o projeto em contÃªineres
â”‚â”€â”€ docs/                    # DocumentaÃ§Ã£o do projeto
â”‚   â”œâ”€â”€ autoavaliacao.md      # AutoavaliaÃ§Ã£o detalhada
â”‚   â”œâ”€â”€ estrutura_dados.md    # Justificativa sobre armazenamento SQL/NoSQL/Data Lake
â”‚â”€â”€ sql/                     # Scripts SQL para criaÃ§Ã£o e manipulaÃ§Ã£o de tabelas
â”‚   â”œâ”€â”€ criar_tabelas.sql     # Script para criar tabelas no SQL Server
â”‚   â”œâ”€â”€ inserir_dados_sql.py  # Script para inserir dados no banco de dados
â”‚   â”œâ”€â”€ query_economistas.sql # Consulta SQL otimizada para economistas
â”‚â”€â”€ .gitignore                # Arquivos que nÃ£o devem ser versionados
â”‚â”€â”€ README.md                 # DocumentaÃ§Ã£o principal do projeto
```

---

## **âš™ï¸ ConfiguraÃ§Ã£o do Ambiente**
### **1ï¸âƒ£ Instalar dependÃªncias**
```bash
# Criar e ativar o ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate  # Windows

# Instalar as bibliotecas necessÃ¡rias
pip install -r requirements.txt
```

### **2ï¸âƒ£ Criar as tabelas no SQL Server**
```bash
# Rodar o script para criar as tabelas necessÃ¡rias
psql -U meu_usuario -d minha_base -f sql/criar_tabelas.sql
```

### **3ï¸âƒ£ Executar o pipeline de ETL**
```bash
# Executar o script de ETL no Apache Airflow
python airflow/scripts/etl_pipeline.py
```

---

## **ğŸ“Š Justificativa para Uso de Data Lake, SQL ou NoSQL**

Para a estruturaÃ§Ã£o e armazenamento dos dados da ANTAQ, foi adotada uma abordagem hÃ­brida, combinando Data Lake, SQL Server e NoSQL (MongoDB). Cada tecnologia foi escolhida estrategicamente para atender Ã s necessidades de armazenamento, processamento e consulta dos dados, garantindo flexibilidade e eficiÃªncia.

### 1. Data Lake (Armazenamento Bruto)
Motivo: O Data Lake foi utilizado para armazenar os dados brutos extraÃ­dos da ANTAQ, garantindo que os cientistas de dados tenham acesso Ã  informaÃ§Ã£o completa para anÃ¡lises exploratÃ³rias e modelagens avanÃ§adas.
BenefÃ­cios:
Capacidade de armazenar grandes volumes de dados em diferentes formatos (estruturados, semiestruturados e nÃ£o estruturados).
Permite anÃ¡lise retrospectiva dos dados sem perda de informaÃ§Ãµes.
Facilita a reprocessamento de dados conforme necessÃ¡rio.
### 2. SQL Server (Dados Estruturados)
Motivo: O SQL Server foi escolhido para armazenar os dados jÃ¡ transformados e estruturados, garantindo alto desempenho para consultas analÃ­ticas realizadas por analistas e economistas.
BenefÃ­cios:
Facilita a indexaÃ§Ã£o e otimizaÃ§Ã£o das consultas.
Garante integridade referencial e consistÃªncia dos dados.
Permite integraÃ§Ã£o direta com ferramentas de BI, como Power BI.
### 3. NoSQL (MongoDB - Dados Semi-Estruturados)
Motivo: O MongoDB foi utilizado para armazenar metadados e logs relacionados Ã s cargas, proporcionando maior flexibilidade para consultas dinÃ¢micas e acesso rÃ¡pido a informaÃ§Ãµes nÃ£o estruturadas.
BenefÃ­cios:
Melhor desempenho para armazenamento de documentos JSON (logs, status de processamento, metadados de cargas).
Flexibilidade para armazenar diferentes formatos de dados sem necessidade de um esquema rÃ­gido.
Facilidade na escalabilidade horizontal, permitindo crescimento eficiente.
### ConclusÃ£o

| Tipo de Armazenamento | Motivo |
|----------------------|--------|
| **Data Lake** | Armazena dados brutos para anÃ¡lises exploratÃ³rias. |
| **SQL Server** | Organiza os dados estruturados para consultas rÃ¡pidas e eficientes. |
| **NoSQL (MongoDB)** | Melhor para armazenar dados semi-estruturados, como logs e metadados de carga. |

---

## **ğŸš€ Executando com Docker e Apache Airflow**
Caso prefira rodar o projeto com **Docker e Airflow**, siga os passos abaixo:

### **1ï¸âƒ£ Criar os contÃªineres**
```bash
docker-compose up -d
```

### **2ï¸âƒ£ Acessar a interface do Airflow**
```bash
http://localhost:8080
```
- **Login:** admin
- **Senha:** admin

---

## **ğŸ“ˆ Consultas SQL para AnÃ¡lise dos Economistas**
Os economistas precisam de uma consulta otimizada para exportar dados ao **Excel** sem sobrecarregar a ferramenta. O script SQL para esta anÃ¡lise estÃ¡ no arquivo [`sql/query_economistas.sql`](sql/query_economistas.sql).

---

## **âœ… ConclusÃ£o**
Este projeto entrega um **pipeline de ETL completo** para ingestÃ£o, transformaÃ§Ã£o e armazenamento dos dados do **AnuÃ¡rio EstatÃ­stico da ANTAQ**, utilizando **Python, Pyspark, SQL Server e Apache Airflow**.

**ğŸ’¡ Tecnologias utilizadas:**  
âœ… **Python**  
âœ… **PySpark**  
âœ… **SQL Server**  
âœ… **Apache Airflow**  
âœ… **Docker**  
âœ… **Power BI**  

Este repositÃ³rio representa minha soluÃ§Ã£o para o desafio de **Especialista em Engenharia de Dados do ObservatÃ³rio da IndÃºstria**. ğŸš€ğŸ’¡

